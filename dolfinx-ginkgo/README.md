# dolfinx-ginkgo

GPU-accelerated distributed linear solvers for DOLFINx using Ginkgo.

[![Build Status](https://img.shields.io/badge/build-passing-brightgreen)]()
[![Ginkgo](https://img.shields.io/badge/Ginkgo-1.11.0-blue)]()
[![DOLFINx](https://img.shields.io/badge/DOLFINx-0.9.0-blue)]()

## Overview

dolfinx-ginkgo provides an alternative linear solver backend for DOLFINx/FEniCSx applications, enabling GPU-accelerated sparse linear algebra with:

- **CUDA** (NVIDIA GPUs)
- **HIP** (AMD GPUs via ROCm)
- **SYCL** (Intel GPUs via oneAPI)
- **OpenMP** (CPU parallelism)

The library integrates with the existing PETSc-based assembly workflow, converting PETSc matrices to Ginkgo's distributed format for GPU-accelerated solving.

## Current Status

| Component | Status | Notes |
|-----------|--------|-------|
| Matrix Conversion | âœ… Complete | PETSc MPIAIJ â†’ Ginkgo distributed::Matrix |
| Vector Conversion | âœ… Complete | PETSc Vec â†” Ginkgo distributed::Vector |
| CG Solver | âœ… Tested | With/without preconditioner |
| GMRES Solver | âœ… Tested | Configurable Krylov dimension |
| Jacobi Preconditioner | âœ… Tested | Point and block variants |
| ILU Preconditioner | âœ… Implemented | Via Schwarz wrapper |
| AMG Preconditioner | âœ… Implemented | PGM coarsening, V/W/F cycles |
| Python Bindings | ğŸš§ Planned | Phase 4 |
| GPU Backends | ğŸš§ Untested | CUDA/HIP/SYCL compile flags ready |

## Quick Start

```bash
# Clone and build with Docker
cd dolfinx-ginkgo
docker build -t dolfinx-ginkgo:latest .

# Run tests
docker run --rm -v "$(pwd):/home/fenics/dolfinx-ginkgo" \
  -w /home/fenics/dolfinx-ginkgo/build dolfinx-ginkgo:latest \
  bash -c "cmake .. -DCMAKE_PREFIX_PATH=/usr/local/dolfinx-real \
           -DDOLFINX_GINKGO_BUILD_PYTHON=OFF && make -j2"

# Test matrix/vector conversion
docker run --rm -v "$(pwd):/home/fenics/dolfinx-ginkgo" \
  -w /home/fenics/dolfinx-ginkgo/build dolfinx-ginkgo:latest \
  mpirun -n 2 ./tests/test_distributed_matrix

# Test solvers
docker run --rm -v "$(pwd):/home/fenics/dolfinx-ginkgo" \
  -w /home/fenics/dolfinx-ginkgo/build dolfinx-ginkgo:latest \
  mpirun -n 2 ./tests/test_solver
```

## Features

### Krylov Solvers
- Conjugate Gradient (CG)
- Flexible CG (FCG)
- GMRES
- BiCGSTAB
- CGS

### Preconditioners
- Point Jacobi
- Block Jacobi
- ILU (Incomplete LU)
- IC (Incomplete Cholesky)
- ISAI (Approximate Sparse Inverse)
- **AMG** (Algebraic Multigrid with PGM coarsening)

### Key Capabilities
- Distributed computing via MPI
- Automatic PETSc â†’ Ginkgo matrix/vector conversion
- AMG preconditioner with configurable cycle types (V, W, F)
- Mixed-precision AMG support
- Python bindings via nanobind

## Requirements

### Required
- CMake â‰¥ 3.19
- Ginkgo â‰¥ 1.11.0 (with distributed support and MPI)
- DOLFINx â‰¥ 0.9.0
- PETSc (for matrix assembly and conversion)
- MPI

### Optional
- CUDA Toolkit â‰¥ 11.0 (for NVIDIA GPUs)
- ROCm â‰¥ 4.5 (for AMD GPUs)
- oneAPI â‰¥ 2023.1 (for Intel GPUs)
- nanobind (for Python bindings)
- Google Test (for unit tests)

## Installation

### Option 1: Docker (Recommended)

The easiest way to build and test is using the provided Docker setup:

```bash
cd cardioEMI/dolfinx-ginkgo

# Build Docker image and run tests
./docker-build.sh
```

This builds a Docker image with DOLFINx 0.9.0 + Ginkgo 1.11.0, compiles the library, and runs the tests.

To use interactively:

```bash
# Build the image
docker build -t dolfinx-ginkgo .

# Run container
docker run -it -v "$(pwd)/..:/home/fenics" -w /home/fenics dolfinx-ginkgo bash
```

### Option 2: Native Installation

Requires Ginkgo 1.8.0+ installed on your system.

```bash
cd cardioEMI/dolfinx-ginkgo

# Create build directory
mkdir build && cd build

# Configure (adjust options as needed)
cmake .. \
    -DCMAKE_BUILD_TYPE=Release \
    -DDOLFINX_GINKGO_ENABLE_CUDA=ON \
    -DDOLFINX_GINKGO_BUILD_PYTHON=ON \
    -DDOLFINX_GINKGO_BUILD_TESTS=ON

# Build
make -j$(nproc)

# Install
make install

# Run tests
ctest
```

### CMake Options

| Option | Default | Description |
|--------|---------|-------------|
| `DOLFINX_GINKGO_ENABLE_CUDA` | OFF | Enable CUDA backend |
| `DOLFINX_GINKGO_ENABLE_HIP` | OFF | Enable HIP backend |
| `DOLFINX_GINKGO_ENABLE_SYCL` | OFF | Enable SYCL backend |
| `DOLFINX_GINKGO_BUILD_PYTHON` | ON | Build Python bindings |
| `DOLFINX_GINKGO_BUILD_TESTS` | ON | Build tests |
| `DOLFINX_GINKGO_BUILD_EXAMPLES` | ON | Build examples |

## Usage

### Python

```python
from dolfinx_ginkgo import GinkgoSolver

# After assembling matrix A with DOLFINx/multiphenicsx
solver = GinkgoSolver(
    A,
    comm=comm,
    backend="cuda",       # or "hip", "omp"
    solver="cg",
    preconditioner="amg",
    rtol=1e-8,
    amg_config={
        "max_levels": 10,
        "cycle": "v",
        "smoother": "jacobi",
        "coarse_solver": "direct"
    }
)

# Solve
for timestep in range(num_timesteps):
    # ... assemble RHS b ...
    solver.solve(b, x)
    print(f"Converged in {solver.iterations} iterations")
```

### C++

```cpp
#include <dolfinx_ginkgo/ginkgo.h>

namespace dgko = dolfinx_ginkgo;

// Create executor and communicator
auto exec = dgko::create_executor(dgko::Backend::CUDA, 0);
auto gko_comm = dgko::create_communicator(MPI_COMM_WORLD);

// Configure solver
dgko::SolverConfig config;
config.solver = dgko::SolverType::CG;
config.preconditioner = dgko::PreconditionerType::AMG;
config.rtol = 1e-8;
config.amg.cycle = dgko::AMGConfig::Cycle::V;

// Create distributed matrix from PETSc
auto A_gko = dgko::create_distributed_matrix_from_petsc<>(exec, gko_comm, A);

// Create solver
dgko::DistributedSolver<> solver(exec, gko_comm, config);
solver.set_operator(A_gko);

// Solve
auto b_gko = dgko::create_distributed_vector_from_petsc<>(exec, gko_comm, b);
auto x_gko = dgko::create_distributed_vector_from_petsc<>(exec, gko_comm, x);
solver.solve(*b_gko, *x_gko);

// Copy result back
dgko::copy_to_petsc(*x_gko, x);
```

## Architecture

```
DOLFINx/multiphenicsx Assembly â†’ PETSc Matrix â†’ Extract CSR â†’ Ginkgo dist::Matrix â†’ Ginkgo Solver
                                  (per rank)      (per rank)     (distributed)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         MPI Rank 0                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Assembly    â†’   PETSc Mat   â†’   Extract CSR   â†’   Ginkgo Matrix â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†• MPI Communication â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         MPI Rank 1                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Assembly    â†’   PETSc Mat   â†’   Extract CSR   â†’   Ginkgo Matrix â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## File Structure

```
dolfinx-ginkgo/
â”œâ”€â”€ CMakeLists.txt
â”œâ”€â”€ Dockerfile                    # DOLFINx 0.9.0 + Ginkgo 1.11.0
â”œâ”€â”€ docker-build.sh               # Build and test script
â”œâ”€â”€ README.md
â”œâ”€â”€ cmake/
â”‚   â””â”€â”€ dolfinx_ginkgo-config.cmake.in
â”œâ”€â”€ cpp/
â”‚   â””â”€â”€ dolfinx_ginkgo/
â”‚       â”œâ”€â”€ ginkgo.h              # Main header, Backend enum, SolverConfig, AMGConfig
â”‚       â”œâ”€â”€ Partition.h           # IndexMap â†’ Ginkgo Partition
â”‚       â”œâ”€â”€ convert.h             # PETSc CSR extraction (MPIAIJ support)
â”‚       â”œâ”€â”€ DistributedMatrix.h   # PETSc Mat â†’ Ginkgo distributed::Matrix
â”‚       â”œâ”€â”€ DistributedVector.h   # PETSc Vec â†’ Ginkgo distributed::Vector
â”‚       â””â”€â”€ Solver.h              # DistributedSolver with Krylov solvers + preconditioners
â”œâ”€â”€ python/
â”‚   â””â”€â”€ dolfinx_ginkgo/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ _cpp.cpp              # nanobind bindings (planned)
â”‚       â””â”€â”€ solver.py             # High-level Python API (planned)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ CMakeLists.txt
â”‚   â”œâ”€â”€ test_partition.cpp        # Partition unit tests
â”‚   â”œâ”€â”€ test_convert.cpp          # CSR conversion unit tests
â”‚   â”œâ”€â”€ test_distributed_matrix.cpp  # MPI matrix/vector integration tests
â”‚   â””â”€â”€ test_solver.cpp           # Solver integration tests (CG, GMRES, preconditioners)
â””â”€â”€ examples/
    â”œâ”€â”€ CMakeLists.txt
    â””â”€â”€ poisson.cpp               # Example Poisson solve
```

## AMG Configuration

The AMG preconditioner uses Ginkgo's PGM (Parallel Graph Match) coarsening:

| Parameter | Default | Description |
|-----------|---------|-------------|
| `max_levels` | 10 | Maximum multigrid levels |
| `min_coarse_rows` | 100 | Stop coarsening below this size |
| `cycle` | V | Cycle type: V, W, or F |
| `smoother` | JACOBI | Smoother: JACOBI, GAUSS_SEIDEL, ILU |
| `pre_smooth_steps` | 1 | Pre-smoothing iterations |
| `post_smooth_steps` | 1 | Post-smoothing iterations |
| `relaxation_factor` | 0.9 | Smoother relaxation |
| `coarse_solver` | DIRECT | Coarse solver: DIRECT, CG, GMRES |
| `use_mixed_precision` | false | Enable mixed precision |

## Performance Notes

- **Setup phase** (matrix conversion, AMG hierarchy): Done once
- **Solve phase**: GPU-accelerated, runs every timestep
- **Memory transfers**: b and x vectors transferred each timestep

For best performance:
1. Use AMG for large-scale problems (mesh-independent convergence)
2. Keep matrix structure fixed when possible (use `update_operator()`)
3. Consider mixed-precision AMG for additional speedup

## Test Results

The following tests pass on 2 MPI ranks:

```
=== Test: Distributed Matrix Conversion ===
--- Test 1: CSR Extraction --- [OK]
--- Test 2: Ginkgo Matrix Creation --- [OK]
--- Test 3: Ginkgo Vector Creation --- [OK]
--- Test 4: Vector Round-Trip --- [OK]

=== Test: Distributed Solver ===
--- Test: CG (no preconditioner) ---
  Iterations: 100, error = 5.99e-15 [OK]

--- Test: CG + Jacobi ---
  Iterations: 100, error = 5.99e-15 [OK]

--- Test: CG + Block Jacobi ---
  Iterations: 52, error = 9.67e-13 [OK]

--- Test: Ginkgo vs PETSc comparison ---
  PETSc:  100 iterations, error = 2.89e-15
  Ginkgo: 100 iterations, error = 5.99e-15 [OK]
```

## License

MIT License

## References

- [Ginkgo Documentation](https://ginkgo-project.github.io/)
- [DOLFINx Documentation](https://docs.fenicsproject.org/dolfinx/main/python/)
- [Ginkgo Distributed Examples](https://github.com/ginkgo-project/ginkgo/tree/develop/examples/distributed)
